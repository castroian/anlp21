{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b90b391844a0>:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  n, dimension = glove2word2vec(glove_file, original_file)\n"
     ]
    }
   ],
   "source": [
    "glove_file=\"../data/glove.6B.100d.100K.txt\"\n",
    "original_file=\"../data/glove.6B.100d.100K.w2v.txt\"\n",
    "n, dimension = glove2word2vec(glove_file, original_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(original_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Implement the Lesk algorithm using word vectors \\([Basile et al. 2014](https://www.aclweb.org/anthology/C/C14/C14-1151.pdf)\\), where we measure the similarity between a gloss g = $\\{ g_1, \\ldots, g_G \\}$ and context c = $\\{ c_1, \\ldots, c_C \\}$ as the cosine similarity between the sum of distributed representations:\n",
    "\n",
    "$$\n",
    "\\cos \\left(\\sum_{i=1}^G g_i, \\sum_{i=1}^C c_i  \\right)\n",
    "$$\n",
    "\n",
    "* The gloss for a synset can be found in `synset.definition()`; be sure to tokenize it appropriately.  \n",
    "* You can find the cosine *distance* (not similarity) between two vectors using the `scipy.spatial.distance.cosine(vector_one, vector_two)` function.\n",
    "* `wn.synsets(word, pos=part_of_speech)` gets you a list of the synsets for a word with a specific part of speech (e.g., \"n\" for noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence, part_of_speech):\n",
    "    # Find all possible word senses\n",
    "    possible_senses = wn.synsets(word, pos=part_of_speech)\n",
    "    \n",
    "    # tokenize the target sentence\n",
    "    sent_tokens = nltk.tokenize.word_tokenize(sentence.lower()) \n",
    "    sent_vec = np.ones(dimension) * 0\n",
    "    \n",
    "    # under Lesk algo, sum the vector representations for the words in the sent\n",
    "    for sent_token in sent_tokens:\n",
    "        sent_vec = sent_vec + wv[sent_token]\n",
    "        \n",
    "    # Setting up the comparisons\n",
    "    min_dist = float(\"inf\")\n",
    "    best_sense = None\n",
    "    \n",
    "    for sense in possible_senses:\n",
    "        # For each sense, tokenize the gloss\n",
    "        sense_rep = nltk.tokenize.word_tokenize(sense.definition().lower())\n",
    "        sense_vec = np.ones(dimension) * 0\n",
    "        \n",
    "        # Create the sum of vector representations for words in the gloss\n",
    "        for sense_token in sense_rep:\n",
    "            sense_vec = sense_vec + wv[sense_token]\n",
    "        \n",
    "        # Cosine distance between both vectors\n",
    "        sense_to_sent_dist = cosine(sense_vec, sent_vec)\n",
    "        \n",
    "        # Opposite of similarity; therefore, lower distance = more similar\n",
    "        if sense_to_sent_dist < min_dist:\n",
    "            # If similar, set the smallest distance and create a new best sense\n",
    "            min_dist = sense_to_sent_dist\n",
    "            best_sense = sense\n",
    "    \n",
    "    print(\"The best synset is\", best_sense, \"\\n\")\n",
    "    print(\"WordNet definition:\", best_sense.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following two cells to check whether your implementation distinguishes between these two senses of \"bank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best synset is Synset('depository_financial_institution.n.01') \n",
      "\n",
      "WordNet definition: a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "lesk(\"bank\", \"I deposited my money into my savings account at the bank\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best synset is Synset('bank.n.07') \n",
      "\n",
      "WordNet definition: a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n"
     ]
    }
   ],
   "source": [
    "lesk(\"bank\", \"I ran along the river bank\", \"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the implementation does distinguish between the two senses of \"bank.\" However, while it does distinguish between the two senses, it does seem to struggle with finding the \"correct\" definition, particularly with the \"I ran along the river bank\" example. It looks like the inclusion of \"I ran\" confuses the algorithm, since it assumes that it's referring to a sport or athlete (who might run on a road or track) instead of land by water (which would be flagged by river). Removing that phrase does seem to create the correct definition, however -- maybe we would need to weight various words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best synset is Synset('bank.n.01') \n",
      "\n",
      "WordNet definition: sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "# Adding this to see if it can differentiate\n",
    "# One issue: it looks like the phrase \"I ran\" leads to the wrong definition\n",
    "lesk(\"bank\", \"along the river bank\", \"n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
