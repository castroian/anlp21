{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for comparing word probabilities in two corpora\n",
    "* it incorporates prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement both of these ratios and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator, math, nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        tokens=[]\n",
    "        # lowercase\n",
    "        for line in file:\n",
    "            data=line.rstrip().lower()\n",
    "            # This dataset is already tokenized, so we can split on whitespace\n",
    "            tokens.extend(data.split(\" \"))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use in this case comes from a sample of 1000 positive and 1000 negative movie reviews from the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).  The version of the data used in this homework has already been tokenized for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens=read_and_tokenize(\"../data/negative.reviews.txt\")\n",
    "positive_tokens=read_and_tokenize(\"../data/positive.reviews.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.  Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "Here the two corpora are the positive movie reviews (e.g., $i$ = positive) and the negative movie reviews (e.g., $j$ = negative). Using this metric, print out the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(tokens):\n",
    "    # From Dr. Bamman's ChiSquare, Mann-Whitney Test notebook\n",
    "    counts=Counter()\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "    return counts\n",
    "\n",
    "def find_vocab(one_tokens, two_tokens):\n",
    "    # Finds all unique words as a list\n",
    "    one = list(one_tokens)\n",
    "    two = list(two_tokens)\n",
    "    return list(set(one + two))\n",
    "\n",
    "import numpy as np\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    vocabulary = find_vocab(one_tokens, two_tokens) # finds all unique words as a list\n",
    "    \n",
    "    V = len(vocabulary) # vocab size V \n",
    "    adjustment = 0.01 # adjustment term aw, using value above\n",
    "    alpha = adjustment * V # a nought\n",
    "    ni = len(one_tokens) # num words in corpus i, set as one\n",
    "    nj = len(two_tokens) # num words in corpus j, set as two\n",
    "    counter_i = get_counts(one_tokens) \n",
    "    counter_j = get_counts(two_tokens) \n",
    "    \n",
    "    results = {} # log_odds storage\n",
    "    for word in vocabulary: # word = w\n",
    "        word_count_i = counter_i[word] # yiw, num times word appears in i\n",
    "        word_count_j =counter_j[word] # yjw, num times word appears in j\n",
    "        log_i = np.log((word_count_i + adjustment) / (ni + alpha - word_count_i - adjustment))\n",
    "        log_j = np.log((word_count_j + adjustment) / (nj + alpha - word_count_j - adjustment))\n",
    "\n",
    "        # Calculating log odds\n",
    "        numer = log_i - log_j\n",
    "        denom = (1 / (word_count_i + adjustment)) + (1 / (word_count_j + adjustment))\n",
    "        log_odds = numer / np.sqrt(denom)\n",
    "        \n",
    "        results[word] = log_odds\n",
    "    \n",
    "    # From Bamman's notebook:\n",
    "    sorted_logs = sorted(results.items(), key=operator.itemgetter(1), reverse=True) # sort the values\n",
    "    positive_words = sorted_logs[:display] \n",
    "    negative_words = sorted_logs[-display:][::-1] # flip it \n",
    "    \n",
    "    print(str(display) + \" words most strongly aligned with first corpus:\\n\")\n",
    "    for word, odds in positive_words:\n",
    "        print(\"%s\\t%s\" % (word, odds))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(str(display) + \" words most strongly aligned with second corpus:\\n\")\n",
    "    for word, odds in negative_words:\n",
    "        print(\"%s\\t%s\" % (word, odds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 words most strongly aligned with first corpus:\n",
      "\n",
      "great\t9.607540224421118\n",
      "his\t8.444585050679482\n",
      "best\t8.221173530354823\n",
      "as\t8.068276896558293\n",
      "and\t8.008455816960856\n",
      "love\t7.4664889052138665\n",
      "war\t7.231860280397061\n",
      "excellent\t7.142854492584097\n",
      "wonderful\t6.697252740688556\n",
      "is\t6.559130822066367\n",
      "her\t6.38883111352343\n",
      "performance\t6.052211880700309\n",
      ",\t5.93698228878491\n",
      "of\t5.768768034806312\n",
      "life\t5.7217812496838585\n",
      "highly\t5.706899505669689\n",
      "world\t5.664462780541281\n",
      "perfect\t5.547139675935874\n",
      "in\t5.489693080913076\n",
      "always\t5.465521212926001\n",
      "performances\t5.380094126983327\n",
      "beautiful\t5.3556137393424645\n",
      "most\t5.198384926387417\n",
      "tony\t5.148053450218753\n",
      "loved\t5.0921802161916085\n",
      "\n",
      "\n",
      "25 words most strongly aligned with second corpus:\n",
      "\n",
      "bad\t-15.874118374895222\n",
      "?\t-15.035079097668289\n",
      "n't\t-11.949393783552106\n",
      "movie\t-10.959996673992299\n",
      "worst\t-9.92867459130721\n",
      "i\t-9.448181178355652\n",
      "just\t-9.122270515824324\n",
      "...\t-8.675997956464013\n",
      "was\t-8.617584504048052\n",
      "no\t-7.999249396143025\n",
      "do\t-7.521169947814628\n",
      "awful\t-7.511891984576927\n",
      "terrible\t-7.446279268276979\n",
      "they\t-7.372767849274727\n",
      "horrible\t-7.052577619117426\n",
      "why\t-7.019505671855516\n",
      "this\t-6.934937867357723\n",
      "poor\t-6.930684966472034\n",
      "boring\t-6.709363182052385\n",
      "any\t-6.684833313059192\n",
      "waste\t-6.674077981733288\n",
      "script\t-6.6612890317425215\n",
      "worse\t-6.6012235452154595\n",
      "have\t-6.55152365358799\n",
      "stupid\t-6.4750566877612865\n"
     ]
    }
   ],
   "source": [
    "logodds_with_uninformative_prior(positive_tokens, negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: As you increase the constant $\\alpha_w$ in the equations above, what happens to $\\hat\\zeta_w^{(i-j)}$, $\\hat{d}_w^{(i-j)}$ and $\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)$ (i.e., do they get bigger or smaller)?  Answer this by plugging the following values in your implementation of these two quantities, and varying $\\alpha_w$ (and, consequently, $\\alpha_0$).\n",
    "\n",
    "* $y_w^i=34$\n",
    "* $y_w^j=17$\n",
    "* $n^i=1000$\n",
    "* $n^j=1000$\n",
    "* $V=500$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values for 0.001 are:\n",
      "d hat = 0.7105541271578355\n",
      "stdev = 0.0882309690871031\n",
      "zeta = 2.3921402672382497\n",
      "\n",
      "The values for 0.01 are:\n",
      "d hat = 0.7102095992733704\n",
      "stdev = 0.08819206440820998\n",
      "zeta = 2.3915077005224097\n",
      "\n",
      "The values for 0.1 are:\n",
      "d hat = 0.7068143816959451\n",
      "stdev = 0.08780504536022363\n",
      "zeta = 2.3853144648767124\n",
      "\n",
      "The values for 1 are:\n",
      "d hat = 0.6765135879981137\n",
      "stdev = 0.08412698412698412\n",
      "zeta = 2.3324313166697372\n",
      "\n",
      "The values for 10 are:\n",
      "d hat = 0.4912029668423381\n",
      "stdev = 0.05976430976430976\n",
      "zeta = 2.0092779913600722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjustment_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "yiw = 34\n",
    "yjw = 17\n",
    "ni = 1000\n",
    "nj = 1000\n",
    "V = 500\n",
    "\n",
    "for adj in adjustment_values:\n",
    "    print(\"The values for \" + str(adj) + \" are:\")\n",
    "    alpha = adj * V\n",
    "    log_i = np.log((yiw + adj) / (ni + alpha - yiw - adj))\n",
    "    log_j = np.log((yjw + adj) / (nj + alpha - yjw - adj))\n",
    "    numer = log_i - log_j\n",
    "    print(\"d hat = \" + str(numer))\n",
    "    denom = (1 / (yiw + adj)) + (1 / (yjw + adj))\n",
    "    print(\"stdev = \" + str(denom))\n",
    "    log_odds = numer / np.sqrt(denom)\n",
    "    print(\"zeta = \" + str(log_odds))\n",
    "    print()\n",
    "    \n",
    "# In general:\n",
    "# As aw increases, \n",
    "# Zeta (log odds with prior likelihood) decreases\n",
    "# d hat (log odds) decreases\n",
    "# standard dev decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make that prior informative by including information about the overall frequency of a given word in a background corpus (i.e., a corpus that represents general word usage, without regard for labeled subcorpora).  To do so, there are only two small changes to make:\n",
    "\n",
    "* We need to gather a background corpus $b$ and calculate $\\hat\\pi_w$, the relative frequency of word $w$ in $b$ (i.e., the number of times $w$ occurs in $b$ divided by the number of words in $b$).\n",
    "\n",
    "* In the uninformative prior above, $\\alpha_w$ was a constant (0.01) and $\\alpha_0 = V * \\alpha_w$.  Let us now set $\\alpha_0 = 1000$ and $\\alpha_w = \\hat\\pi_w * \\alpha_0$.  This reflects a pseudocount capturing the fractional number of times we would expect to see word $w$ in a sample of 1000 words.\n",
    "\n",
    "This allows us to specify that a common word like \"the\" (which has a relative frequency of $\\approx 0.04$) would have $\\alpha_w = 40$, while an infrequent word like \"beneficiaries\" (relative frequency $\\approx 0.00002$) would have $\\alpha_w = 0.02$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement a log-odds ratio with informative prior, using a larger background corpus of 5M tokens drawn from the same dataset (given to you as `priors` below, which contains the relative frequencies of words calculated from that corpus) and set $\\alpha_0 = 1000$. Using this metric, print out again the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus.  Is there a meaningful difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates b\n",
    "def read_priors(filename):\n",
    "    counts=Counter()\n",
    "    freqs={}\n",
    "    tokens=read_and_tokenize(filename)\n",
    "    total=len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "\n",
    "    for word in counts:\n",
    "        freqs[word]=counts[word]/total\n",
    "\n",
    "    return freqs\n",
    "    \n",
    "priors=read_priors(\"../data/sentiment.background.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_informative_prior(one_tokens, two_tokens, priors, display=25):\n",
    "    \n",
    "    vocabulary = find_vocab(one_tokens, two_tokens)\n",
    "    V = len(vocabulary) # vocab size\n",
    "    alpha = 1000\n",
    "    ni = len(one_tokens)\n",
    "    nj = len(two_tokens)\n",
    "    counter_i = get_counts(one_tokens)\n",
    "    counter_j = get_counts(two_tokens)\n",
    "    \n",
    "    results = {} # log_odds storage\n",
    "    for word in vocabulary: # word = w\n",
    "        adjustment = priors[word] * alpha\n",
    "        word_count_i = counter_i[word] # yiw\n",
    "        word_count_j = counter_j[word] # yjw\n",
    "        log_i = np.log((word_count_i + adjustment) / (ni + alpha - word_count_i - adjustment))\n",
    "        log_j = np.log((word_count_j + adjustment) / (nj + alpha - word_count_j - adjustment))\n",
    "\n",
    "        # Calculating log odds\n",
    "        numer = log_i - log_j\n",
    "        denom = (1 / (word_count_i + adjustment)) + (1 / (word_count_j + adjustment))\n",
    "        log_odds = numer / np.sqrt(denom)\n",
    "        \n",
    "        results[word] = log_odds\n",
    "    \n",
    "    # From Bamman's notebook:\n",
    "    sorted_logs = sorted(results.items(), key=operator.itemgetter(1), reverse=True) # sort the values\n",
    "    positive_words = sorted_logs[:25] #[word[0] for word in sorted_logs[:25]]\n",
    "    negative_words = sorted_logs[-25:][::-1] #[word[0] for word in sorted_logs[-25:]]\n",
    "    \n",
    "    print(str(display) + \" words most strongly aligned with first corpus:\\n\")\n",
    "    for word, odds in positive_words:\n",
    "        print(\"%s\\t%s\" % (word, odds))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(str(display) + \" words most strongly aligned with second corpus:\\n\")\n",
    "    for word, odds in negative_words:\n",
    "        print(\"%s\\t%s\" % (word, odds))\n",
    "    \n",
    "\n",
    "    priors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 words most strongly aligned with first corpus:\n",
      "\n",
      "great\t9.591039308150844\n",
      "his\t8.427876598766474\n",
      "best\t8.207390895133855\n",
      "as\t8.051850426312377\n",
      "and\t7.990231407305706\n",
      "love\t7.4541507138623375\n",
      "war\t7.2257038478692515\n",
      "excellent\t7.136307614907904\n",
      "wonderful\t6.692625533714691\n",
      "is\t6.543717647282005\n",
      "her\t6.3771329954200775\n",
      "performance\t6.042737335969907\n",
      ",\t5.9215701353486665\n",
      "of\t5.754969742770618\n",
      "life\t5.712104514477569\n",
      "highly\t5.703588456479672\n",
      "world\t5.6554566648511475\n",
      "perfect\t5.540329074257345\n",
      "in\t5.477091362874526\n",
      "always\t5.456154882740344\n",
      "performances\t5.371497571484572\n",
      "beautiful\t5.346392148729829\n",
      "most\t5.188839322996821\n",
      "tony\t5.151557465549794\n",
      "loved\t5.08520207589855\n",
      "\n",
      "\n",
      "25 words most strongly aligned with second corpus:\n",
      "\n",
      "bad\t-15.858436155657074\n",
      "?\t-15.012171561204545\n",
      "n't\t-11.92973245486743\n",
      "movie\t-10.942156841842946\n",
      "worst\t-9.958385464468778\n",
      "i\t-9.43394122124469\n",
      "just\t-9.10719606286112\n",
      "...\t-8.661491103140472\n",
      "was\t-8.60428995816332\n",
      "no\t-7.98623150105582\n",
      "awful\t-7.513361158186305\n",
      "do\t-7.509005013285732\n",
      "terrible\t-7.450145065620428\n",
      "they\t-7.360873277110952\n",
      "horrible\t-7.054947486972777\n",
      "why\t-7.008333826955033\n",
      "this\t-6.924931742171541\n",
      "poor\t-6.923265303686221\n",
      "waste\t-6.719492793687231\n",
      "boring\t-6.70218207705369\n",
      "any\t-6.674061366310463\n",
      "script\t-6.651941912393764\n",
      "worse\t-6.597130696108337\n",
      "have\t-6.541331663788079\n",
      "stupid\t-6.468495330825354\n"
     ]
    }
   ],
   "source": [
    "logodds_with_informative_prior(positive_tokens, negative_tokens, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
