{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/anlp21/blob/main/16.ie/DependencyPatterns_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwF0eQEuUZDI"
   },
   "source": [
    "This notebook explores relation extraction by measuring common dependency paths between two entities that hold a given relation to each other -- here, the relation \"born_in\" between a PER entity and an GPE entity, using data from Wikipedia biographies.  Feel free to run this notebook either on your on computer or on Google Colab. If using Google Colab, install these libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoU-B7rDUc-L",
    "outputId": "bf82d56b-1482-46e1-e4d0-9b03196b536d"
   },
   "outputs": [],
   "source": [
    "!pip install spacy==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oab0AdCFUuid",
    "outputId": "7c186843-0027-46cd-94d5-e7fa39e43784"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z78f7KFuUwlG",
    "outputId": "897fe36e-fc7d-4e02-ac23-af04da351d7f"
   },
   "outputs": [],
   "source": [
    "!pip install neuralcoref --no-binary neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, use your local `anlp21-spacy2` environment we set up in class on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKOFd6tiUZDK",
    "outputId": "cb4d4a9c-a355-4617-b42a-4dcc13b0cb7f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgsWNpewUZDM"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiO2oWmMUZDM"
   },
   "outputs": [],
   "source": [
    "def get_path(one, two):\n",
    "    \n",
    "    \"\"\" Get dependency path between two tokens in a sentence; return None if not reachable \"\"\"\n",
    "    \n",
    "    one_heads=[x for x in one.ancestors]\n",
    "    two_heads=[x for x in two.ancestors]\n",
    "    \n",
    "    up_path=[]\n",
    "    down_path=[]\n",
    "    up_path.append(one)\n",
    "    down_path.append(two)\n",
    "    \n",
    "    lca=None\n",
    "    for head in one_heads:\n",
    "        if head in two_heads:\n",
    "            lca=head\n",
    "            break\n",
    "            \n",
    "        up_path.append(head)\n",
    "\n",
    "    for head in two_heads:\n",
    "        if head == lca:\n",
    "            break\n",
    "    \n",
    "        down_path.append(head)\n",
    "   \n",
    "    if lca is None:\n",
    "        return None\n",
    "    \n",
    "    path=\"%s->%s<-%s\" % ('->'.join([\"%s\" % x.dep_ for x in up_path]), lca.text, '<-'.join([\"%s\" % x.dep_ for x in reversed(down_path)]))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtyODxkPUZDN"
   },
   "outputs": [],
   "source": [
    "def get_closest_coref(entity1, clusters, target_entity):\n",
    "    \n",
    "    \"\"\" Given entities e1 and mention m2 of another entity, returns the mention for e1 closest to m2 \"\"\"\n",
    "    \n",
    "    targetCluster=None\n",
    "    for chain in clusters:\n",
    "        for mention in chain.mentions:\n",
    "            if mention.start <= entity1.start and mention.end >= entity1.end:\n",
    "                targetCluster=chain\n",
    "                break\n",
    "\n",
    "    if targetCluster is None:\n",
    "        return None\n",
    "\n",
    "    closestMention=None\n",
    "    dist=100\n",
    "    for mention in targetCluster:\n",
    "            sentDist=abs(target_entity.sent.start-mention.sent.start)\n",
    "            if sentDist < dist:\n",
    "                dist=sentDist\n",
    "                closestMention=mention\n",
    "            if sentDist == dist and closestMention is not None:\n",
    "                if abs(target_entity.start-mention.start) < abs(target_entity.start-closestMention.start):\n",
    "                    closetMention=mention\n",
    "    return closestMention\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBQrUEpIUZDO"
   },
   "source": [
    "Q0. In class activity: here's [a Google spreadsheet](https://docs.google.com/spreadsheets/d/1PNDInP5JIqad9mOXwRUxGDZntvoUerX22QQcgFCJDxY/edit?usp=sharing) with the first 5 sentences from ~500 Wikipedia biographies.  If you're present in class, pick 10 rows of this spreadsheet that other students haven't already claimed and put your student ID in the \"Student ID\" column; then go through those 10 rows and read the document. If you can infer that a person from the \"Candidate people\" column was born in a place in the \"Candidate places\" column, list that person in the \"PER BORN\" column and the place in the \"PLACE BORN\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGLebaU1UZDO"
   },
   "outputs": [],
   "source": [
    "def read_training(filename):\n",
    "    \n",
    "    \"\"\" Read in training data for <person, place> tuples that express the \"born_in\" relation.\n",
    "    \n",
    "    -- Use coreference resolution to identity the person mention closest to the place mention.\n",
    "    -- Use dependency parsing to extract the syntactic path from that person mention to the place.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            doc=cols[1]\n",
    "            pers=cols[4]\n",
    "            place=cols[5].rstrip()\n",
    "            \n",
    "            if pers != \"\" and place != \"\":\n",
    "                doc=nlp(doc)\n",
    "\n",
    "                target_person=None\n",
    "                target_place=None\n",
    "                \n",
    "                # Annotations are at the type level, so let's anchor them to specific mentions\n",
    "                for entity in doc.ents:\n",
    "                    if entity.text == pers:\n",
    "                        target_person=entity\n",
    "                    elif entity.text == place:\n",
    "                        target_place=entity\n",
    "                \n",
    "                if target_person is not None and target_place is not None:\n",
    "                    \n",
    "                    # Use coreference to get person mention that's closest to the place (ideally in the same sentence).\n",
    "                    closest_person_mention=get_closest_coref(target_person, doc._.coref_clusters, target_place)\n",
    "                    if closest_person_mention is None:\n",
    "                        closest_person_mention=target_person\n",
    "                    \n",
    "                    path=get_path(closest_person_mention.root, target_place.root)\n",
    "                    \n",
    "                    # if a path can be found between the two\n",
    "                    if path is not None:\n",
    "                        data.append((pers, place, path, target_place.sent ))\n",
    "    return data     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku2ARfoSUZDP"
   },
   "source": [
    "At the end of class, save this Google sheet as a tsv in `born.tsv` and execute the `read_training` function on it to read in the <person, place> tuples.  If you're executing this on Google Colab, download the born.tsv file here (after class).  In both cases, adjust the path to `born.tsv` for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAU-tLEnVTmu",
    "outputId": "085194b2-8cac-4de8-a8bf-1f61a0718f11"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/anlp21/master/data/born.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alBZWUqaUZDP",
    "outputId": "6163cd2b-44e0-482c-ea81-08c1d17d5ea8"
   },
   "outputs": [],
   "source": [
    "trainingData=read_training(\"born.tsv\")\n",
    "for data in trainingData:\n",
    "    print ('\\t'.join([str(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgEV-vuoUZDQ"
   },
   "source": [
    "Q1: Count the syntactic paths identified in the training data.  What are the two that are most frequently attested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--Mj__7qUZDQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IxKi-5IUZDQ"
   },
   "source": [
    "Q2: Write a function to read in a target file (containing one document per line) and a syntactic path and identify all people/places that are joined by that path. Hint: you can use the get_path functon defined above to retrieve the syntactic path between two tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7ar9FHhUZDR"
   },
   "outputs": [],
   "source": [
    "def extract_relations(filename, target_path):\n",
    "    \n",
    "    \"\"\" Extract new relations from a file.\n",
    "    Input: \n",
    "        - filename containing one document per line\n",
    "        - target_path: the syntactic dependency path connecting the person entity to the place entity\n",
    "    Output:\n",
    "        - a list of (person, place, path, sentence) tuples in the same format returned from `read_training`.\n",
    "    \n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYU4EFGKWkwZ"
   },
   "source": [
    "If you're executing this on Google Colab, download the wiki.bio.born.test.txt file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNstowpRVx2x",
    "outputId": "b2e9f7db-6ea3-44dd-a0bb-2076f472682d"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/anlp21/master/data/wiki.bio.born.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z2Sv8zvUZDR"
   },
   "outputs": [],
   "source": [
    "new_examples=extract_relations(\"wiki.bio.born.test.txt\", \"nsubjpass->born<-prep<-pobj\")\n",
    "for data in new_examples:\n",
    "    print ('\\t'.join([str(x) for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIuuEVgJUZDR"
   },
   "source": [
    "Q3: Execute `extract_relations` on `wiki.bio.born.test.txt` and the two most frequent paths identified in the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCpj002OUZDS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DependencyPatterns_TODO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
