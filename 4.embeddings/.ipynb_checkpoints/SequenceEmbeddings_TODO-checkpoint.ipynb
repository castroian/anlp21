{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a41b9af",
   "metadata": {},
   "source": [
    "Children often ask their caregivers to find episodes of their favorite TV shows based only on a very short (and loosely relevant!) description of it (\"the one where Arthur has a wiggly tooth\") but video services like Netflix and Amazon don't currently provide such content-based search. Given summaries of each episode, can we use sequence embeddings to solve this retrieval problem?\n",
    "\n",
    "Before beginning this homework, install the following libraries:\n",
    "\n",
    "```sh\n",
    "conda install -c huggingface transformers\n",
    "pip install -U sentence-transformers\n",
    "conda install -c conda-forge ipywidgets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb5a6e",
   "metadata": {},
   "source": [
    "First, let's read in our data for the TV show \"Wild Kratts\" (from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Wild_Kratts_episodes)), which has the following (tab-separated) form:\n",
    "\n",
    "|Episode|Title|Summary|\n",
    "|:-|:-|:-|\n",
    "|1|Mom of a Croc|At the Nile River, zoologists Chris and Martin Kratt (voiced by their real-world selves) are on a mission to show one of their fellow Wild Kratts team members—brilliant young inventor Aviva Corcovado (Athena Karkanis)—that there's more to crocodiles than just violence and snapping jaws. After shrinking themselves down to a few inches tall by using Aviva's Miniaturizer invention, the Kratt Brothers disguise themselves as crocodile eggs and sneak into a mother crocodile's new nest. In the Wild Kratts team's turtle-shaped aircraft and headquarters—the Tortuga, one of Aviva's greatest inventions—the Wild Kratts tech team, consisting of Aviva, communications expert and mechanic Koki (Heather Bambrick), and skilled pilot Jimmy Z (Jonathan Malen) monitor Chris and Martin and watch as the mother crocodile faithfully guards her nest against predators for months without even eating anything. Eventually, as the crocodile eggs hatch and the crocodile mom uses her mouth to carry several of her newly hatched babies to the river, Aviva changes her mind about crocodiles and decides that these reptiles are in fact caring and dedicated mothers. But when the mother crocodile leaves the river to go get more hatchlings from her nest, predators threaten the first batch of baby crocodiles. The Kratt Brothers must use the incredible Creature Power Suits—two of Aviva's inventions—to gain the abilities of crocodiles and protect the vulnerable crocodile hatchlings.|\n",
    "|2|Whale of a Squid|The Kratt Brothers use Aviva's amphipod-inspired submersible, the Amphisub, to dive into the deep waters of the Southern Ocean. There, they witness a never-before-seen wildlife moment: a battle between a sperm whale and a giant squid. However, the water pressure at the extreme depths where the battle is taking place badly damages and partially crushes the Amphisub, forcing Aviva to use her new ExtendoArm invention to pull the submersible back to the Tortuga. To allow Chris and Martin to return to the site of the whale-versus-squid battle, Aviva programs two new Creature Power Suits—Sperm Whale Power for Chris, and Squid Power for Martin. The Kratt Brothers use their new Creature Powers to dive back into the deep sea, where the sperm whale and the giant squid are still locked in combat. Suddenly, the sperm whale becomes entangled in a discarded fishing net and begins sinking toward an area full of underwater volcanoes. To make matters worse, a colossal squid attacks the sperm whale's calf. Chris and Martin must put their Creature Powers of both sperm whale and squid to good use to rescue the mother sperm whale and her calf.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4afb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            episode=cols[0]\n",
    "            title=cols[1]\n",
    "            summary=cols[2]\n",
    "            data.append((episode, title, summary))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a84627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read_data(\"../data/wild_kratts_episodes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c13e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_reps_for_data(data, sequence_embedding_function, model):\n",
    "    \n",
    "    # This function applies the sequence_embedding_function argument (a function itself) to the summary\n",
    "    # element in the input data list, and returns a copy of that list with an embedding of that summary attached.\n",
    "    \n",
    "    data_with_reps=[]\n",
    "    \n",
    "    for episode, title, summary in data:\n",
    "        data_with_reps.append((episode, title, summary, sequence_embedding_function(model, summary)))\n",
    "    \n",
    "    return data_with_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccd4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f6781",
   "metadata": {},
   "source": [
    "First, we may be tempted to use the [CLS] token for BERT to represent an entire input string (as is often done in *supervised* document classification models).  How well does this work as an out-of-the-box document representation not optimized for our particular task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cecba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39709416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9782f0",
   "metadata": {},
   "source": [
    "**Q1**: Fill out the `get_cls_token_for_doc` function to return the [CLS] embedding for the input string.  The output should be a single 768-dimensional numpy vector (see `4.embeddings/BERT.ipynb` for converting between a pytorch tensor and a numpy object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666851c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_token_for_doc(model, string):\n",
    "    # Adapting code from Dr. Bamman's notebook\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    \n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    term_idx=tokens.index(\"[CLS]\")\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a69de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_cls_data=get_document_reps_for_data(data, get_cls_token_for_doc, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7afe7",
   "metadata": {},
   "source": [
    "**Q2**: Use these representations to find the episode that is most similar to the description \"The one where they bounce back in time\" by having the highest cosine similarity between representations.  A sample function shell `run_query` is provided below, along with the only arguments you need, but feel free to adapt it as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f01193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "# Importing tools for sorting a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864ba3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, data_with_reps, sequence_embedding_function, model):\n",
    "    # query: query string\n",
    "    # data_with_reps: episode number, name, description, numpy matrix\n",
    "    # sequence_embedding_function: returns numpy vector\n",
    "    # model: bert model\n",
    "    \n",
    "    results={}\n",
    "    query_rep = sequence_embedding_function(model, query) \n",
    "    \n",
    "    for sent in data_with_reps:\n",
    "        comp_rep=sequence_embedding_function(model, sent[2])\n",
    "        cos_sim=cosine_similarity(query_rep, comp_rep)\n",
    "        results[sent[1]] = cos_sim\n",
    "        \n",
    "    sorted_vals = sorted(results.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Most Similar Episode Name: \" + sorted_vals[0][0])\n",
    "    print(\"Cosine Similarity to Query:\", sorted_vals[0][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fceee614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Episode Name: Little Howler\n",
      "Cosine Similarity to Query: 0.75525886\n"
     ]
    }
   ],
   "source": [
    "query=\"The one where they bounce back in time\"\n",
    "run_query(query, bert_cls_data, get_cls_token_for_doc, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907772a",
   "metadata": {},
   "source": [
    "Now let's try a sentence embedding model that was optimized for generating sentence representations: Sentence-BERT ([Reimers and Gurevych 2019](https://arxiv.org/pdf/1908.10084.pdf)).  Example usage (in the context of the Huggingface transformers library) can be found [here](https://www.sbert.net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db44111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63c4cd",
   "metadata": {},
   "source": [
    "**Q3**: Fill out the `get_sentence_embedding` function below to return the sentence embedding for the input string, and use it again to find the episode that is most similar to the description \"The one where they bounce back in time\" by having the highest cosine similarity between representations.  Which method for generating sentence embeddings appears better for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae518e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(model, string):\n",
    "    return model.encode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc18d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_data=get_document_reps_for_data(data, get_sentence_embedding, sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6da02ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Episode Name: Back in Creature Time: Day of the Dodo\n",
      "Cosine Similarity to Query: 0.34919497\n"
     ]
    }
   ],
   "source": [
    "query=\"The one where they bounce back in time\"\n",
    "run_query(query, sentence_transformer_data, get_sentence_embedding, sentence_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe1ccd",
   "metadata": {},
   "source": [
    "Sentence embeddings appears to be better for this text. The previous implementation did have a higher cosine similarity, but the episode did not seem to be about time travel (it was about wolves and moose). In this case, even though the cosine similarity is lower, we actually have an episode where there is time travel. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
