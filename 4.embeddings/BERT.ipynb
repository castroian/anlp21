{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792cb96",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  Before using, be sure to install the following libraries:\n",
    "\n",
    "```sh\n",
    "conda install -c conda-forge ipywidgets\n",
    "conda install -c huggingface transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddc388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a12638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bcd6dc34944466aaa1fcce0600a412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c19495e08bf4d03bf1c2bf760816c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514597d638de4ec1a0e906725f1fd68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3613e34dcc534747a7a29f381410ea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e55c8afb7fc481c9f55b3214666d5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f34541",
   "metadata": {},
   "source": [
    "BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"vaccinated\", for instanced, is tokenized as [\"va\", \"##cci\", \"##nated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a1e451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'new',\n",
       " 'data',\n",
       " 'shows',\n",
       " '26',\n",
       " 'states',\n",
       " 'have',\n",
       " 'fully',\n",
       " 'va',\n",
       " '##cci',\n",
       " '##nated',\n",
       " 'more',\n",
       " 'than',\n",
       " 'half',\n",
       " 'their',\n",
       " 'residents',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a390bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'bert',\n",
       " 'is',\n",
       " 'super',\n",
       " '##cal',\n",
       " '##if',\n",
       " '##rag',\n",
       " '##ilis',\n",
       " '##tic',\n",
       " '##ex',\n",
       " '##pia',\n",
       " '##lid',\n",
       " '##oc',\n",
       " '##ious',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a73c94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2023,  9389,  2003, 12090,   102])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b2be",
   "metadata": {},
   "source": [
    "As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c2fa",
   "metadata": {},
   "source": [
    "To generate representations for the input tokens, simply pass the input through the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bc73aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'jam', 'is', 'delicious', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the tokens\n",
    "inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
    "# pass in the tokens to the model\n",
    "outputs = model(**inputs)\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3f309",
   "metadata": {},
   "source": [
    "Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967d8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe68cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888e87d",
   "metadata": {},
   "source": [
    "We'll cover using BERT for supervised problems in later classes, but what can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eccbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b79b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"I ate some jam with toast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc6477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be898d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_for_token(string, term):\n",
    "    \n",
    "    # tokenize\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    \n",
    "    # convert input ids to words\n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
    "    term_idx=tokens.index(term)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # return the BERT rep for that token index\n",
    "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
    "    \n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719345c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "query_rep=get_bert_for_token(query, \"jam\")\n",
    "print(query_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd0e1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\tI ate some jam with toast\tMy grandma makes the best jam.\n",
      "0.665\tI ate some jam with toast\tThis jam is made of strawberries\n",
      "0.480\tI ate some jam with toast\tI sat in a traffic jam for 2 hours\n",
      "0.443\tI ate some jam with toast\tThe Grateful Dead used to jam for like two days straight.\n",
      "0.385\tI ate some jam with toast\tShe got me out of a real jam\n",
      "0.290\tI ate some jam with toast\tI had to jam on the brakes to avoid hitting him.\n"
     ]
    }
   ],
   "source": [
    "vals=[]\n",
    "for sent in comp_sents:\n",
    "    comp_rep=get_bert_for_token(sent, \"jam\")\n",
    "    cos_sim=cosine_similarity(query_rep, comp_rep)\n",
    "    vals.append((cos_sim, query, sent))\n",
    "\n",
    "for c, q, s in reversed(sorted(vals)):\n",
    "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7167cc",
   "metadata": {},
   "source": [
    "**Q**: Brainstorm the variety of things you can do with token-level word representations like this and we'll discuss them at the end of class.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42395734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
