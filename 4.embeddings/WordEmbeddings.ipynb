{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores word embeddings through the functionality of Gensim; we train new embeddings from a dataset of our own and compare with pre-trained Glove embeddings.\n",
    "\n",
    "Before running, install gensim with:\n",
    "\n",
    "`conda install gensim`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's train a new word2vec model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "filename=\"../data/wiki.10K.txt\"\n",
    "with open(filename) as file:\n",
    "    for line in file:\n",
    "        words=line.rstrip().lower()\n",
    "        # this file is already tokenize, so we can split on whitespace\n",
    "        # but first let's replace any sequence of whitespace (space, tab, newline, etc.) with single space\n",
    "        words=re.sub(\"\\s+\", \" \", words)\n",
    "        sentences.append(words.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        sentences,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.6040468e+00,  4.6519479e-01,  1.3723152e+00,  2.4232153e-02,\n",
       "        3.6689457e-01, -2.0964488e-01, -6.3129492e-02,  1.7304672e+00,\n",
       "       -6.9785982e-01, -2.4084120e+00,  4.4760983e-02,  8.6913890e-01,\n",
       "       -3.6593404e-01, -2.9760936e-01,  8.3753347e-01, -1.5732206e+00,\n",
       "        1.0369208e+00, -2.8990591e-01, -1.4570014e+00, -2.8426056e+00,\n",
       "        1.5044382e+00, -3.1966957e-01,  8.1602246e-01,  8.2970345e-01,\n",
       "        5.3989303e-01, -8.2801718e-01,  1.1680928e+00,  8.2889128e-01,\n",
       "       -2.7990193e+00, -1.0557247e+00,  1.8167909e+00,  1.1370176e+00,\n",
       "        1.6069021e+00,  6.9361746e-01,  1.6337723e+00, -1.2583779e+00,\n",
       "        1.2168442e+00, -2.5753796e+00, -1.3693327e+00,  1.1552408e+00,\n",
       "       -8.0097437e-01, -1.4968573e+00,  8.7179160e-01, -5.0303859e-01,\n",
       "        2.8146556e-01, -1.3122885e+00, -7.6302958e-01,  1.0624727e+00,\n",
       "        5.2453274e-01, -1.2822746e+00, -5.0274181e-01,  3.5793233e-01,\n",
       "       -1.7118424e+00, -1.5503944e+00,  4.3970987e-01,  3.2836959e-01,\n",
       "       -1.2578715e-01, -8.1559151e-02, -1.2064580e+00, -2.3169458e+00,\n",
       "        8.2594961e-01,  1.5133977e+00,  3.6695769e-01,  5.9467340e-01,\n",
       "        1.3942961e+00, -9.2438090e-01,  3.8514552e-01,  2.0109580e+00,\n",
       "        8.9379424e-01, -1.7262666e-01,  1.4953946e+00, -6.2710541e-01,\n",
       "       -1.7476915e+00, -1.4771472e+00, -1.8725224e+00, -1.8993845e-02,\n",
       "        5.2917939e-01,  1.0370373e+00,  1.8160824e-01,  7.0805955e-01,\n",
       "        8.9586246e-01, -1.7338665e-01,  6.2800127e-01,  1.8899257e+00,\n",
       "       -6.3365549e-01,  2.0336926e+00,  7.2854853e-01, -1.7889756e-03,\n",
       "       -5.2915889e-01, -2.1307311e+00, -1.2962337e+00, -1.0247326e+00,\n",
       "       -1.5706991e-01, -8.2542807e-01, -9.9209189e-01, -5.5987805e-01,\n",
       "       -3.6276761e-01, -2.5191662e+00, -5.6539077e-01,  3.6669764e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_vectors = model.wv\n",
    "model.wv\n",
    "# save vectors to file if you want to use them later\n",
    "#my_trained_vectors.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.9563195109367371),\n",
       " ('musician', 0.9091904759407043),\n",
       " ('composer', 0.9039162993431091),\n",
       " ('producer', 0.8941104412078857),\n",
       " ('writer', 0.8919810056686401),\n",
       " ('artist', 0.8903812170028687),\n",
       " ('dancer', 0.8714725971221924),\n",
       " ('singer', 0.864169180393219),\n",
       " ('pianist', 0.8626704812049866),\n",
       " ('journalist', 0.8625409007072449)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_vectors.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in vectors that have already been trained on a much bigger dataset. [Glove vectors](https://nlp.stanford.edu/projects/glove/) are trained using a different method than word2vec, but results in vectors that can be read in by Gensim.  Here we'll use a 100-dimensional model trained on 6B words (from Wikipedia and news), but bigger models are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-86a990e6d4aa>:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, glove_in_w2v_format)\n"
     ]
    }
   ],
   "source": [
    "# First we have to convert the Glove format into w2v format; this creates a new file\n",
    "glove_file=\"../data/glove.6B.100d.100K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.6B.100d.100K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fb53d8614c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(\"../data/glove.6B.100d.100K.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8580666184425354),\n",
       " ('comedian', 0.795758843421936),\n",
       " ('starring', 0.7920297384262085),\n",
       " ('starred', 0.7582033276557922),\n",
       " ('actors', 0.7394535541534424),\n",
       " ('filmmaker', 0.7349801063537598),\n",
       " ('screenwriter', 0.7342271208763123),\n",
       " ('film', 0.6941469311714172),\n",
       " ('movie', 0.6924506425857544),\n",
       " ('comedy', 0.6884662508964539)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"actor\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`most_similar` allows for vector arithmetic (as the average value of the input positive/negative vectors, where negative vectors are first multiplied by -1).  Play around with this function to discover other analogies that have been learned in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('germany', 0.892362117767334),\n",
       " ('austria', 0.7597677111625671),\n",
       " ('poland', 0.7425415515899658),\n",
       " ('denmark', 0.7360999584197998),\n",
       " ('german', 0.6986513733863831)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one + two = three + ?\n",
    "one=\"man\"\n",
    "two=\"king\"\n",
    "three=\"woman\"\n",
    "\n",
    "one=\"paris\"\n",
    "two=\"france\"\n",
    "three=\"berlin\"\n",
    "\n",
    "glove.most_similar(positive=[two, three], negative=[one], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the quality of the learned vectors through an intrinsic evaluation comparing to human judgments in the wordsim 353 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.5483502278951546, 4.235096667699337e-29),\n",
       " SpearmanrResult(correlation=0.5327354323238274, pvalue=2.8654146580558905e-27),\n",
       " 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.38471142639602574, 1.0135344149176915e-13),\n",
       " SpearmanrResult(correlation=0.39151377703291373, pvalue=3.399532939047208e-14),\n",
       " 1.41643059490085)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_trained_vectors.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
