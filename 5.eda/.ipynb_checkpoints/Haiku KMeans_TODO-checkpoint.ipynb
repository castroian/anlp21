{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8780af9",
   "metadata": {},
   "source": [
    "In \"[Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/full/10.1086/684353)\", Long and So train a classifier to differentiate haiku poems from non-haiku poems, and find that many features help do so.  In class, we've discussed the importance of representation--how you *describe* a text computationally influences the kinds of things you are able to do with it.  While Long and So explore description in the context of classification, in this homework, you'll see how well you can design features that can differentiate these two classes *without* any supervision. Are you able to featurize a collection of poems such that two clusters (haiku/non-haiku) emerge when using KMeans clustering, with the text representation as your only degree of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f517024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(path, metadata, filepath_col):\n",
    "    data=[]\n",
    "    with open(metadata, encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for cols in csv_reader:\n",
    "            poem_path=os.path.join(path, cols[filepath_col])\n",
    "            if os.path.exists(poem_path):\n",
    "                with open(poem_path, encoding=\"utf-8\") as poem_file:\n",
    "                    poem=poem_file.read()\n",
    "                    data.append(poem)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81026d2",
   "metadata": {},
   "source": [
    "Here we'll use data originally released on Github to support \"Literary Pattern Recognition\": [https://github.com/hoytlong/PatternRecognition](https://github.com/hoytlong/PatternRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbe9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku=read_texts(\"../data/haiku/long_so_haiku\", \"../data/haiku/Haikus.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336544fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "others=read_texts(\"../data/haiku/long_so_others\", \"../data/haiku/OthersData.csv\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change anything within this code block\n",
    "\n",
    "def run_all(haiku, others, feature_function):\n",
    "    \n",
    "    X, Y, featurize_vocab=feature_function(haiku, others)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c16b1",
   "metadata": {},
   "source": [
    "As one example, let's take a simple featurization and represent each poem by a binary indicator of the dictionary word types it contains.  \"To be or not to be\", for example, would be represented as {\"to\": 1, \"be\": 1, \"or\": 1, \"not\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f433360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a list of haiku poems and non-haiku poems, and returns:\n",
    "\n",
    "# X (sparse matrix, with poems as rows and features as columns)\n",
    "# Y (list of poem labels, with 1=haiku and 0=non-haiku)\n",
    "# feature_vocab (dict mapping feature name to feature ID)\n",
    "\n",
    "def unigram_featurize_all(haiku, others):\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab: # check if token is in feature_vocab \n",
    "                feature_vocab[token]=len(feature_vocab) # if not, add it to the vocab\n",
    "            feats[feature_vocab[token]]=1 # then, add it to feats dict with binary 1/0\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695d6fb",
   "metadata": {},
   "source": [
    "This method yields an NMI of ~0.07 (with some variability due to the randomness of KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b420ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, unigram_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57de6",
   "metadata": {},
   "source": [
    "**Q1**: Copy the `unigram_featurize_all` code above and adapt it to create your own featurization method named `fancy_featurize_all`.  You may use whatever information you like to represent these poems for the purposes of clustering them into two categories, but you must use the KMeans clustering (with 2 clusters) as defined in `run_all`.  Use your own understanding of haiku, or read the Long and So article above for other ideas.  Are you able to improve over an NMI of 0.07?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4bb27af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for embedding\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "be2e46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding approach\n",
    "\n",
    "def fancy_featurize_all(haiku, others):\n",
    "    \n",
    "    sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "    \n",
    "    poems = haiku + others\n",
    "    Y = [0] * len(haiku) + [1] * len(others) \n",
    "        \n",
    "    data = sentence_model.encode(poems)\n",
    "    \n",
    "    feature_vocab = {}\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y)) # zip into 2 item list\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "    \n",
    "    return data, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "68479e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.120 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d5d55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for first-try approach\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "from g2p_en import G2p\n",
    "g2p = G2p()\n",
    "\n",
    "def get_pronunciation(word):\n",
    "    if word in arpabet:\n",
    "        # pick the first pronunciation\n",
    "        return arpabet[word][0]\n",
    "\n",
    "    else:\n",
    "        return g2p(word)\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    pronunciation=get_pronunciation(word)\n",
    "    sylls=0\n",
    "    for phon in pronunciation:\n",
    "        # vowels in arpabet end in digits (indicating stress)\n",
    "        if re.search(\"\\d$\", phon) is not None:\n",
    "            sylls+=1\n",
    "    return sylls\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "def spacy_tokenizer(data):\n",
    "    spacy_tokens=nlp(data)\n",
    "    return [token for token in spacy_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "4fb50cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-try approach\n",
    "def fancy_featurize_all(haiku, others):\n",
    "    \n",
    "    def fancy_featurize(poem, feature_vocab):\n",
    "\n",
    "            lowered = poem.lower()\n",
    "            tokens = spacy_tokenizer(lowered)\n",
    "            cleaned_tokens = []\n",
    "            for word in tokens:\n",
    "                if str(word) not in stop_words:\n",
    "                    if word.pos_ == \"NOUN\":\n",
    "                        cleaned_tokens.append(str(word.lemma_))\n",
    "                    elif word.is_punct == False and \"\\n\" not in str(word):\n",
    "                        cleaned_tokens.append(str(word))\n",
    "\n",
    "            feats = {}\n",
    "\n",
    "            text_length = len(cleaned_tokens)\n",
    "            syllables = get_syllable_count(lowered)\n",
    "            feats[0] = text_length\n",
    "            feats[1] = syllables\n",
    "\n",
    "            for token in cleaned_tokens:\n",
    "                if token not in feature_vocab: # check if token is in feature_vocab \n",
    "                    feature_vocab[token]=len(feature_vocab) # if not, add it to the vocab\n",
    "                if feature_vocab[token] not in feats: \n",
    "                    feats[feature_vocab[token]] = 1 \n",
    "                else:\n",
    "                    feats[feature_vocab[token]] = feats[feature_vocab[token]] + 1\n",
    "\n",
    "            keys_to_drop = []\n",
    "            for key in feats:\n",
    "                if feats[key] < 2:\n",
    "                    keys_to_drop.append(key)\n",
    "\n",
    "            [feats.pop(key) for key in keys_to_drop]\n",
    "            return feats\n",
    "    \n",
    "    feature_vocab={\"length_\": 0, \"syllables_\": 1} \n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=fancy_featurize(poem, feature_vocab) \n",
    "        data.append(feats) \n",
    "        Y.append(1) \n",
    "    for poem in others:\n",
    "        feats=fancy_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y)) # zip into 2 item list\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    \n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab))) # convert to matrix\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    \n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a06afb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feda2b4",
   "metadata": {},
   "source": [
    "**Q2**: Describe your method for featurization in 100 words and why you expect it to be able to separate haiku poems from non-haiku poems in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f9b52",
   "metadata": {},
   "source": [
    "My method for featurization utilizes a SentenceTransformer to encode each haiku. As described by [Reimers and Gurevych](https://arxiv.org/abs/1908.10084), this model utilizes Sentence-BERT which utilizes a modified version of BERT to generate sentence embeddings, representing the sentence by mapping it to a vector space.\n",
    "\n",
    "In this case, I expect this approach to separate haikus from non-haikus because it takes into account the context of the entire poem, as compared to simply analyzing the presence or absence of word tokens. As Long and So discuss in their paper, the embedding approach allows for analysis of key factors, such as sentence structure and common themes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d4184",
   "metadata": {},
   "source": [
    "(Note: I included my initial approach above for my own record, which tried to build on the first approach by incorporating features that Long and So discuss, such as syllable count and poem length, with similar cleaning such as lemmatizing nouns and removing stop words. This was much less successful -- it did much worse, and I assume that this is because there are significant overlaps in the haikus and poems in terms of the added features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14a12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
